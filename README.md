# allabaoutllm

- Derivative
- Perceptron
- Backpropagation
- Bigram
- Special chars
- Distribution Sampling algo
- Broadcast
- Loss
- Smoothing
- MLP
- mini-batching
- learning rate
- Weight init
    - Kaiming
    - Xavier
- Batch normalization
- Layer norm
- Instance norm
- RMS norm
- LLM 
    - Bigram (one character predicts the next one with a lookup table of counts)
    - MLP, [Bengio et al. 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
    - CNN, [DeepMind WaveNet](https://arxiv.org/pdf/1609.03499)
    - RNN, [Mikolov et al. 2010](https://www.fit.vut.cz/research/group/speech/public/publi/2010/mikolov_interspeech2010_IS100722.pdf)
    - LSTM, [Graves et al. 2014](https://arxiv.org/pdf/1308.0850)
    - GRU, [Kyunghyun Cho et al. 2014](https://arxiv.org/pdf/1409.1259)
    - Transformer [Vaswani et al. 2017](https://arxiv.org/pdf/1706.03762)

